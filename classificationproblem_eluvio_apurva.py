# -*- coding: utf-8 -*-
"""ClassificationProblem_Eluvio_Apurva.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15UhcJqErWrYFpFufcp1m_0GR0E-15BOu

#Eluvio Data Science Challenge

* The goal of this NLP project is to classify if the title is a high rated news or not.

* I will be using the columns up_votes and title from the dataset

* The up_votes greater than quantile 0.5 are high rated i.e; 1 and those lesser than that are low rated news i.e; 0

## Mounting the drive
"""

from google.colab import drive
drive.mount('/content/drive')

"""## Import the modules"""

#modules for Tokenizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
#modules for plotting
import matplotlib.pyplot as plt
from tensorflow import keras
#modules for math
import pandas as pd
import json
import numpy as np
#modules for modeling
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, mean_absolute_error

"""## Read the data"""

url = "/content/drive/MyDrive/Eluvio- Data Science Intern/Data/Eluvio_DS_Challenge.csv" 
df = pd.read_csv(url, header = 0) 
df = pd.DataFrame(df)
df.head()

df.info()

#check that no columns have null values
df.isnull().sum()

"""## Exploring Columns

* The column 'category' has only one value - worldnews
* The column 'down_votes' has all Zeros
* Lets look at top 5 authors who received highest up_votes
"""

top5_authors = df.nlargest(5, ['up_votes'])
top5_authors[['author','up_votes']].reset_index(drop=True)

"""* Here are the 5 authors with highest up_votes
  * KRISHNA53
  * joeyoungblood
  * mister_geaux
  * navysealassulter
  * seapiglet

## Data Pre-processing

* Since the columns **up_votes** and **title** have a strong correlation, we will classify the title into **high rated** and **low rated** based on up_votes.
* The title with top 50 percentile of the up_votes is considered to be **high rated** and the other is **low rated**
* The high rated group is labelled 1 and the low rated group is 0
"""

high_rate = np.quantile(df['up_votes'], 0.5)
df['news_rating'] = np.where(df['up_votes']>= high_rate, 1, 0)
df.head()

#convert title to lower case
df['title'] = df['title'].str.lower()
df.head()

# replace any special characters with a blank
df['title'] = df['title'].str.replace('[^A-z ]','').str.replace(' +',' ').str.strip()
df.head()

"""* Lets look at a graph for the new column "news_rating" that we created based on up_votes
* We have a balanced dataset for both the groups
"""

df['news_rating'].value_counts()

# plotting
import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(10,5))
ax = sns.countplot(x="news_rating", data=df,
                   order = df['news_rating'].value_counts().index)
ax.set_xticklabels(ax.get_xticklabels(), rotation = 90)
plt.show()

"""### Tokenizer

* The tokenizer takes a given sentence and parses it into a list with individual words separated by a comma. This makes it easier for us to do some feature engineering

### Stop Words

* I introduce stop words to improve the performance of the models and remove them from title

* I downloaded the stopwords from nltk.corpus and extracted the stop words 

* I then applied these stop words to our data to see if the models behave any differently

* I also created some EDA to observe top 20 words with their frequencies

* I plotted frequency distribution graph and word cloud to visualy display the top words
"""

# let's remove some of the stop words
# define some stop words
import nltk
from nltk.corpus import stopwords

nltk.download('stopwords') 
print(stopwords.words('english'))

stop = stopwords.words('english')

df.head() # note how the stop words have been removed
df["title"] = df['title'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)])) 
df.head()

"""### EDA"""

TopWords = df['title'].str.split(expand=True).stack().value_counts()
TopWords[0:20]

# we can make a frequency plot 
# we'll group, then convert to pandas DataFrame for easy plotting
x = df['title'].str.split(expand=True).stack().value_counts()
x = pd.DataFrame(x)
# reset the index
x.reset_index(inplace=True)
# rename the columns
x.rename(columns={x.columns[0]:'word', x.columns[1]:'frequency'}, inplace=True)
x.head()

# Frequency Distribution Plot
# look at first X words
x = x[0:20]
x.plot(x='word', y='frequency')
plt.xticks(np.arange(len(x)), x['word'], rotation=90)
plt.suptitle('Top Words in News Title')
plt.show()

from wordcloud import WordCloud

# Generate a word cloud image
wordcloud = WordCloud().generate(' '.join(df['title']))

# Display the generated image:
wordcloud = WordCloud(max_font_size=40).generate(' '.join(df['title']))
plt.figure()
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.show()

"""### Tokenize title"""

# import
nltk.download('punkt')
from nltk.tokenize import word_tokenize

#tokenize title using nltk.word
df['title'] = df.apply(lambda row: nltk.word_tokenize(row['title']), axis=1)
df.head()

"""### Delete unwanted columns"""

# delete the columns other than 'title' and 'news_rating'
del df['author']
del df['category']
del df['date_created']
del df['down_votes']
del df['over_18']
del df['time_created']
del df['up_votes']

df.head()

"""### Rejoin the words in title to prepare data for Modeling"""

def rejoin_words(row):
    my_list = row['title']
    joined_words = ( " ".join(my_list))
    return joined_words

df['title'] = df.apply(rejoin_words, axis=1)
# here it is after - no square brackets and commas!
df['title'].head()

"""## Splitting the data into X and y

* Splitting the dataset into X and y
* 80/20 split into train and test samples 
"""

X = pd.DataFrame(df['title'])
y = df['news_rating']

X.head()

print(X.shape)
print(y.shape)

from sklearn.feature_extraction.text import CountVectorizer
from nltk.tokenize import RegexpTokenizer

# get rid of garbage text
token = RegexpTokenizer(r'[a-zA-Z0-9]+')

#tokenizer to remove unwanted elements from out data like symbols and numbers
cv = CountVectorizer(lowercase=True,
                     stop_words='english',
                     #using unigram
                     ngram_range = (1,1), # (1,1) is unigram, (1,2) is uni and bigram, (2,2) is just bigram
                     tokenizer = token.tokenize)
text_counts= cv.fit_transform(X['title']) 
print(text_counts.shape)

# inspect text_counts
# check out the first row
print(text_counts)
np.sum(text_counts[1]) # number of entries

# partition into 80/20 train and test samples 
X_train, X_test, y_train, y_test = train_test_split(text_counts, y, test_size=0.2, random_state=1)

print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)

"""## Modeling

### Decision Tree Classifier
"""

from sklearn.tree import DecisionTreeClassifier
#Import scikit-learn metrics module for accuracy calculation
from sklearn import metrics
# Model Generation Using Decision Tree Classifier
clf = DecisionTreeClassifier().fit(X_train, y_train)
predicted= clf.predict(X_test)
print("Decision Tree Classifier Accuracy:",metrics.accuracy_score(y_test, predicted))

"""#### Evaluate the Model
* Classification Report 
* Confusion Matrix
"""

# classification report
from sklearn.metrics import classification_report
print(classification_report(y_test, predicted))

# confusion matrix
from sklearn.metrics import confusion_matrix
confusion_matrix(y_test, predicted)

"""### Logistic Regression"""

from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
#Import scikit-learn metrics module for accuracy calculation
from sklearn import metrics
# Model Generation Using Logistic Regression
clf = LogisticRegression(max_iter = 1000).fit(X_train,y_train)
predicted= clf.predict(X_test)
print("Logistic Regression Accuracy:",metrics.accuracy_score(y_test, predicted))

"""#### Evaluate the Model
* Classification Report 
* Confusion Matrix
"""

from sklearn import metrics
from sklearn.metrics import accuracy_score

#Classification_report
classification_report = metrics.classification_report(y_test,predicted)
print(classification_report)

#Confusion matrix
confusion_matrix = metrics.confusion_matrix(y_test,predicted)
print(confusion_matrix)

# top left is TN
# bottom left is FN
# top right is FP
# bottom right is TP

"""### Multinomial Naive Bayes"""

from sklearn.naive_bayes import MultinomialNB
clf = MultinomialNB().fit(X_train,y_train)
predicted = clf.predict(X_test)
print("Multinomial Naive Bayes Accuracy:",metrics.accuracy_score(y_test, predicted))

"""#### Evaluate the Model
* Classification Report 
* Confusion Matrix
"""

# classification report
from sklearn.metrics import classification_report
print(classification_report(y_test, predicted))

# confusion matrix
from sklearn.metrics import confusion_matrix
confusion_matrix(y_test, predicted)

"""## Create features for Modeling using TF-IDF method

* Since the models are not performing that well, I wanted to see if TF-IDF method to create the features would improve the performance of the models
* TF-IDF is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.
* TF(w) = (Number of times term w appears in a document) / (Total number of terms in the document)
* IDF(w) = log_e(Total number of documents / Number of documents with term w in it)
*TF-IDF (term frequency, inverse document frequency) is the product of these two quantities
* TF-IDF(Term Frequency-Inverse Document Frequency) normalizes the document term matrix. It is the product of TF and IDF. Word with high tf-idf in a document, it is most of the times occurred in given documents and must be absent in the other documents. So the words must be a signature word.

"""

# create the matrix
from sklearn.feature_extraction.text import TfidfVectorizer
tf=TfidfVectorizer()
text_tf= tf.fit_transform(X['title'])

# check out what you did
text_tf.shape

print(text_tf[0])

# split the data
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(text_tf, y, 
                                                    test_size=0.2, random_state=123)

"""### Decision Tree Classifier"""

# fit a model
from sklearn.tree import DecisionTreeClassifier
from sklearn import metrics
# Model Generation Using Multinomial Naive Bayes
clf = DecisionTreeClassifier().fit(X_train, y_train)
predicted= clf.predict(X_test)
print("Decision Tree Classifier Accuracy:",metrics.accuracy_score(y_test, predicted))

"""####Evaluate the Model
* Classification Report
* Confusion Matrix
"""

# classification report
from sklearn.metrics import classification_report
print(classification_report(y_test, predicted))

# confusion matrix
from sklearn.metrics import confusion_matrix
confusion_matrix(y_test, predicted)

"""### Logistic Regression"""

from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
#Import scikit-learn metrics module for accuracy calculation
from sklearn import metrics
# Model Generation Using Logistic Regression
clf = LogisticRegression(max_iter = 1000).fit(X_train,y_train)
predicted= clf.predict(X_test)
print("Logistic Regression Accuracy:",metrics.accuracy_score(y_test, predicted))

"""#### Evaluate the Model
* Classification Report 
* Confusion Matrix
"""

from sklearn import metrics
from sklearn.metrics import accuracy_score

#Classification_report
classification_report = metrics.classification_report(y_test,predicted)
print(classification_report)

#Confusion matrix
confusion_matrix = metrics.confusion_matrix(y_test,predicted)
print(confusion_matrix)

# top left is TN
# bottom left is FN
# top right is FP
# bottom right is TP

"""### Multinomial Naive Bayes"""

from sklearn.naive_bayes import MultinomialNB
clf = MultinomialNB().fit(X_train,y_train)
predicted = clf.predict(X_test)
print("Multinomial Naive Bayes Accuracy:",metrics.accuracy_score(y_test, predicted))

"""#### Evaluate the Model
* Classification Report 
* Confusion Matrix
"""

# classification report
from sklearn.metrics import classification_report
print(classification_report(y_test, predicted))

# confusion matrix
from sklearn.metrics import confusion_matrix
confusion_matrix(y_test, predicted)

"""## Model Comparison

**Model**                                     |    **Accuracy**        |
----------------------------------------------|------------------------|
**Model 1: Decision Tree Classifier**         |      0.5488            |
**Model 2: Logistic Regression**              |      0.5933            |
**Model 3: Multinomial Naive Bayes**          |      0.5850            |
**Model 4: Decision Tree Classifier(TF-IDF)** |      0.5420            |
**Model 5: Logistic Regression(TF-IDF)**      |      0.6044            |
**Model 6: Multinomial Naive Bayes(TF-IDF)**  |      0.5866            |

I decided the best model based on Accuracy, Complexity and Explanability

### Accuracy
Top 3 models based on Accuracy are as follows:

**Model**                                     |    **Accuracy**        |
----------------------------------------------|------------------------|
**Model 5: Logistic Regression(TF-IDF)**      |      0.6044            |
**Model 2: Logistic Regression**              |      0.5933            |
**Model 6: Multinomial Naive Bayes(TF-IDF)**  |      0.5866            |

### Complexity
* I used two methods for feature extraction- Tokenizer and TF-IDF.
Logistic Regression with TF-IDF method for creating features performed the best with 60.44% accuracy rate followed by Logistic Regression with Tokenizer and Multinomial Naive Bayes (TF-IDF) with an accuracy of 59.33% and 58.66% respectively.
* Overall, I observed that TF-IDF method of creating features for models yielded better accuracy rates. However, Decision Tree Classifier performed better with Tokenizer method with an accuracy of 54.88%.
* I also noticed that Decision Tree Classifier models took longer to run when compared to Logistic Regression and Multinomial Naive Bayes models, making the other two more attractive and time consuming.

## Conclusion
* As the dataset is large, I used the power of Tokenizer that makes it easier for feature engineering.
* I also used TF-IDF method which is a glorified form of feature engineering to create features for modeling.
* I got the best model when I used TF-IDF method on Logistic Regression with max_iterations = 1000 which gave an accuracy rate of 60.44%
* Overall, the models had better performance when TF-IDF method was used for feature engineering
* The title is the best column to classify if the news has received more number of up_votes or not.
"""